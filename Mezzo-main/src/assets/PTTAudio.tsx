import { useState, useRef, useEffect } from 'react';
import { Mic, MicOff, Phone, PhoneOff, Volume2, VolumeX } from 'lucide-react';

interface PTTAudioProps {
    deviceId: string;
    channel: string;
    onAudioSend: (audioData: ArrayBuffer, isPrivate: boolean, targetId?: string, transcript?: string) => void;
    onSpeechToText?: (text: string) => void;  // èªéŸ³è½‰æ–‡å­—å›èª¿ï¼ˆå³æ™‚é¡¯ç¤ºç”¨ï¼‰
}

interface AudioPacket {
    id: string;
    type: 'speech' | 'private';
    channel: string;
    from: string;
    audioData: string;  // base64
    timestamp: string;
    randomId?: string;
}

const PTTAudio = ({ deviceId, channel, onAudioSend, onSpeechToText }: PTTAudioProps) => {
    // éŒ„éŸ³ç‹€æ…‹
    const [isRecording, setIsRecording] = useState(false);
    const [isMuted, setIsMuted] = useState(false);
    const [audioLevel, setAudioLevel] = useState(0);

    // ç§äººé€šè©±ç‹€æ…‹
    const [privateCallActive, setPrivateCallActive] = useState(false);
    const [privateTargetId, setPrivateTargetId] = useState('');
    const [randomCallId, setRandomCallId] = useState('');

    // èªéŸ³è½‰æ–‡å­—ç‹€æ…‹
    const [autoSend, setAutoSend] = useState(false);  // è‡ªå‹•ç™¼é€é–‹é—œï¼ˆé è¨­é—œé–‰ï¼‰
    const [silenceThreshold, setSilenceThreshold] = useState(0.02);  // éœéŸ³é–¾å€¼
    const [silenceDuration, setSilenceDuration] = useState(1500);  // éœéŸ³æŒçºŒæ™‚é–“ (ms)
    const [speechRecognitionEnabled, setSpeechRecognitionEnabled] = useState(false);  // STT å¯ç”¨æ€§
    const [currentTranscript, setCurrentTranscript] = useState('');  // ç•¶å‰è­˜åˆ¥çš„æ–‡å­—

    // Refs
    const mediaRecorderRef = useRef<MediaRecorder | null>(null);
    const audioContextRef = useRef<AudioContext | null>(null);
    const analyserRef = useRef<AnalyserNode | null>(null);
    const audioChunksRef = useRef<Blob[]>([]);
    const streamRef = useRef<MediaStream | null>(null);
    const silenceTimerRef = useRef<NodeJS.Timeout | null>(null);
    const recognitionRef = useRef<any>(null);  // Web Speech API
    const isRecordingRef = useRef<boolean>(false);  // éŒ„éŸ³ç‹€æ…‹çš„ ref
    const finalTranscriptRef = useRef<string>('');  // ç´¯ç©çš„æœ€çµ‚è½‰éŒ„æ–‡å­—

    // åˆå§‹åŒ–éŸ³è¨Šä¸Šä¸‹æ–‡å’ŒèªéŸ³è­˜åˆ¥
    useEffect(() => {
        audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)();

        // åˆå§‹åŒ– Web Speech API (å¦‚æœç€è¦½å™¨æ”¯æ´)
        if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
            const SpeechRecognition = (window as any).webkitSpeechRecognition || (window as any).SpeechRecognition;
            recognitionRef.current = new SpeechRecognition();
            recognitionRef.current.continuous = true;
            recognitionRef.current.interimResults = true;
            recognitionRef.current.lang = 'zh-TW';  // ç¹é«”ä¸­æ–‡

            recognitionRef.current.onstart = () => {
                console.log('âœ… Speech recognition started successfully');
                setSpeechRecognitionEnabled(true);
            };

            recognitionRef.current.onend = () => {
                console.log('â¹ï¸ Speech recognition ended');
                setSpeechRecognitionEnabled(false);

                // å¦‚æœæ­£åœ¨éŒ„éŸ³ï¼Œè‡ªå‹•é‡æ–°å•Ÿå‹•èªéŸ³è­˜åˆ¥
                // é€™æ¨£å¯ä»¥ä¿æŒæŒçºŒè­˜åˆ¥
                if (isRecordingRef.current) {
                    setTimeout(() => {
                        try {
                            if (recognitionRef.current && isRecordingRef.current) {
                                recognitionRef.current.start();
                                console.log('ğŸ”„ Speech recognition auto-restarted');
                            }
                        } catch (err) {
                            console.warn('âš ï¸ Auto-restart failed:', err);
                        }
                    }, 100);
                }
            };

            recognitionRef.current.onresult = (event: any) => {
                // æ”¶é›†ç•¶å‰é€™æ¬¡çš„çµæœ
                let interimTranscript = '';

                for (let i = event.resultIndex; i < event.results.length; i++) {
                    const transcript = event.results[i][0].transcript;
                    if (event.results[i].isFinal) {
                        // ç´¯ç©æœ€çµ‚çµæœ
                        finalTranscriptRef.current += transcript;
                        console.log('âœ… Final result added:', transcript);
                    } else {
                        // è‡¨æ™‚çµæœ
                        interimTranscript += transcript;
                    }
                }

                // é¡¯ç¤ºçš„æ–‡å­— = å·²ç¢ºèªçš„æ–‡å­— + è‡¨æ™‚æ–‡å­—
                const displayText = finalTranscriptRef.current + interimTranscript;
                console.log('ğŸ¤ Speech recognized:', {
                    finalAccumulated: finalTranscriptRef.current || '(none)',
                    interim: interimTranscript || '(none)',
                    display: displayText
                });

                // å³æ™‚æ›´æ–°é¡¯ç¤º
                setCurrentTranscript(displayText);

                // é€šçŸ¥çˆ¶çµ„ä»¶ï¼ˆåƒ…ç”¨æ–¼å³æ™‚é¡¯ç¤ºï¼Œä¸å½±éŸ¿ç™¼é€ï¼‰
                if (onSpeechToText && displayText) {
                    onSpeechToText(displayText);
                }
            };

            recognitionRef.current.onerror = (event: any) => {
                console.error('âŒ Speech recognition error:', event.error);
                if (event.error === 'not-allowed') {
                    alert('è«‹å…è¨±éº¥å…‹é¢¨æ¬Šé™ä»¥ä½¿ç”¨èªéŸ³è½‰æ–‡å­—åŠŸèƒ½');
                }
            };

            console.log('âœ… Speech Recognition API initialized');
        } else {
            console.warn('âš ï¸ Web Speech API not supported in this browser');
        }

        return () => {
            if (audioContextRef.current) {
                audioContextRef.current.close();
            }
            if (recognitionRef.current) {
                recognitionRef.current.stop();
            }
        };
    }, [onSpeechToText]);

    // ç›£æ§éŸ³è¨Šé›»å¹³å’ŒéœéŸ³åµæ¸¬
    useEffect(() => {
        if (!isRecording || !analyserRef.current) return;

        const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);
        const updateLevel = () => {
            if (!analyserRef.current) return;

            analyserRef.current.getByteFrequencyData(dataArray);
            const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
            const level = average / 255;
            setAudioLevel(level);

            // éœéŸ³åµæ¸¬ (è‡ªå‹•æ–·å¥)
            if (autoSend && !privateCallActive) {
                if (level < silenceThreshold) {
                    // åµæ¸¬åˆ°éœéŸ³ï¼Œé–‹å§‹è¨ˆæ™‚
                    if (!silenceTimerRef.current) {
                        silenceTimerRef.current = setTimeout(() => {
                            console.log('ğŸ”‡ Silence detected, auto-sending audio chunk...');
                            stopGroupRecording();  // è‡ªå‹•åœæ­¢ä¸¦ç™¼é€
                        }, silenceDuration);
                    }
                } else {
                    // æœ‰è²éŸ³ï¼Œæ¸…é™¤è¨ˆæ™‚å™¨
                    if (silenceTimerRef.current) {
                        clearTimeout(silenceTimerRef.current);
                        silenceTimerRef.current = null;
                    }
                }
            }

            if (isRecording) {
                requestAnimationFrame(updateLevel);
            }
        };

        updateLevel();

        return () => {
            if (silenceTimerRef.current) {
                clearTimeout(silenceTimerRef.current);
                silenceTimerRef.current = null;
            }
        };
    }, [isRecording, autoSend, silenceThreshold, silenceDuration, privateCallActive]);

    // é–‹å§‹ç¾¤çµ„éŒ„éŸ³
    const startGroupRecording = async () => {
        try {
            // æ¸…ç©ºä¹‹å‰ç´¯ç©çš„è½‰éŒ„æ–‡å­—
            finalTranscriptRef.current = '';
            setCurrentTranscript('');

            const stream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true,
                    sampleRate: 16000  // 16kHz é©åˆèªéŸ³
                }
            });

            streamRef.current = stream;

            // è¨­ç½®éŸ³è¨Šåˆ†æå™¨
            if (audioContextRef.current) {
                const source = audioContextRef.current.createMediaStreamSource(stream);
                analyserRef.current = audioContextRef.current.createAnalyser();
                analyserRef.current.fftSize = 256;
                source.connect(analyserRef.current);
            }

            // å‰µå»º MediaRecorder
            const mediaRecorder = new MediaRecorder(stream, {
                mimeType: 'audio/webm;codecs=opus'
            });

            mediaRecorderRef.current = mediaRecorder;
            audioChunksRef.current = [];

            mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    audioChunksRef.current.push(event.data);
                }
            };

            mediaRecorder.onstop = async () => {
                const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
                const arrayBuffer = await audioBlob.arrayBuffer();

                // ä½¿ç”¨ç•¶å‰é¡¯ç¤ºçš„æ–‡å­—ï¼ˆåŒ…å«æœ€çµ‚å’Œè‡¨æ™‚çµæœï¼‰
                // å› ç‚ºæœ‰æ™‚å€™èªéŸ³è­˜åˆ¥é‚„æ²’ä¾†å¾—åŠæ¨™è¨˜ç‚º final å°±è¢«åœæ­¢äº†
                const textToSend = currentTranscript || finalTranscriptRef.current;
                console.log('ğŸ“ Sending audio with transcript:', {
                    currentTranscript,
                    finalTranscript: finalTranscriptRef.current,
                    willSend: textToSend || '(empty)'
                });
                onAudioSend(arrayBuffer, false, undefined, textToSend);

                // æ¸…ç©ºè½‰éŒ„æ–‡å­—
                setCurrentTranscript('');
                finalTranscriptRef.current = '';

                // æ¸…ç†
                audioChunksRef.current = [];
                if (streamRef.current) {
                    streamRef.current.getTracks().forEach(track => track.stop());
                    streamRef.current = null;
                }
            };

            mediaRecorder.start(100);  // æ¯ 100ms æ”¶é›†ä¸€æ¬¡æ•¸æ“š
            setIsRecording(true);
            isRecordingRef.current = true;

            // å•Ÿå‹•èªéŸ³è­˜åˆ¥
            if (recognitionRef.current) {
                try {
                    // å…ˆå˜—è©¦åœæ­¢ï¼ˆå¦‚æœæ­£åœ¨é‹è¡Œï¼‰
                    if (speechRecognitionEnabled) {
                        recognitionRef.current.stop();
                    }
                    // ç­‰å¾…ä¸€ä¸‹å†å•Ÿå‹•
                    setTimeout(() => {
                        try {
                            recognitionRef.current.start();
                            console.log('ğŸ¤ Starting speech recognition...');
                        } catch (err) {
                            console.warn('âš ï¸ Speech recognition start failed:', err);
                        }
                    }, 100);
                } catch (err) {
                    console.warn('âš ï¸ Speech recognition stop failed:', err);
                }
            } else {
                console.warn('âš ï¸ Speech recognition not initialized');
            }

            console.log('ğŸ™ï¸ Started group recording');

        } catch (error) {
            console.error('âŒ Failed to start recording:', error);
            alert('ç„¡æ³•è¨ªå•éº¥å…‹é¢¨ï¼Œè«‹ç¢ºä¿å·²æˆäºˆæ¬Šé™');
        }
    };

    // åœæ­¢ç¾¤çµ„éŒ„éŸ³
    const stopGroupRecording = () => {
        if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
            mediaRecorderRef.current.stop();
            setIsRecording(false);
            isRecordingRef.current = false;
            setAudioLevel(0);

            // åœæ­¢èªéŸ³è­˜åˆ¥
            if (recognitionRef.current) {
                try {
                    recognitionRef.current.stop();
                    console.log('ğŸ¤ Speech recognition stopped');
                } catch (err) {
                    console.warn('âš ï¸ Speech recognition stop failed:', err);
                }
            }

            // æ¸…é™¤éœéŸ³è¨ˆæ™‚å™¨
            if (silenceTimerRef.current) {
                clearTimeout(silenceTimerRef.current);
                silenceTimerRef.current = null;
            }

            console.log('ğŸ™ï¸ Stopped group recording');
        }
    };

    // é–‹å§‹ç§äººé€šè©±
    const startPrivateCall = async () => {
        if (!privateTargetId.trim()) {
            alert('è«‹è¼¸å…¥ç›®æ¨™è¨­å‚™ ID');
            return;
        }

        // æ¸…ç©ºä¹‹å‰ç´¯ç©çš„è½‰éŒ„æ–‡å­—
        finalTranscriptRef.current = '';
        setCurrentTranscript('');

        // ç”Ÿæˆéš¨æ©Ÿé€šè©± ID
        const callId = `CALL-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
        setRandomCallId(callId);
        setPrivateCallActive(true);

        try {
            const stream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true,
                    sampleRate: 16000
                }
            });

            streamRef.current = stream;

            // è¨­ç½®éŸ³è¨Šåˆ†æå™¨
            if (audioContextRef.current) {
                const source = audioContextRef.current.createMediaStreamSource(stream);
                analyserRef.current = audioContextRef.current.createAnalyser();
                analyserRef.current.fftSize = 256;
                source.connect(analyserRef.current);
            }

            const mediaRecorder = new MediaRecorder(stream, {
                mimeType: 'audio/webm;codecs=opus'
            });

            mediaRecorderRef.current = mediaRecorder;
            audioChunksRef.current = [];

            mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    audioChunksRef.current.push(event.data);
                }
            };

            mediaRecorder.onstop = async () => {
                const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
                const arrayBuffer = await audioBlob.arrayBuffer();

                // ä½¿ç”¨ç•¶å‰é¡¯ç¤ºçš„æ–‡å­—ï¼ˆåŒ…å«æœ€çµ‚å’Œè‡¨æ™‚çµæœï¼‰
                const textToSend = currentTranscript || finalTranscriptRef.current;
                console.log('ğŸ“ Sending private audio with transcript:', {
                    currentTranscript,
                    finalTranscript: finalTranscriptRef.current,
                    willSend: textToSend || '(empty)'
                });
                onAudioSend(arrayBuffer, true, callId, textToSend);

                // æ¸…ç©ºè½‰éŒ„æ–‡å­—
                setCurrentTranscript('');
                finalTranscriptRef.current = '';

                audioChunksRef.current = [];
                if (streamRef.current) {
                    streamRef.current.getTracks().forEach(track => track.stop());
                    streamRef.current = null;
                }
            };

            mediaRecorder.start(100);
            setIsRecording(true);
            isRecordingRef.current = true;
            console.log(`ğŸ“ Started private call: ${callId} â†’ ${privateTargetId}`);

        } catch (error) {
            console.error('âŒ Failed to start private call:', error);
            alert('ç„¡æ³•è¨ªå•éº¥å…‹é¢¨');
            setPrivateCallActive(false);
        }
    };

    // çµæŸç§äººé€šè©±
    const endPrivateCall = () => {
        if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
            mediaRecorderRef.current.stop();
        }
        setIsRecording(false);
        isRecordingRef.current = false;
        setPrivateCallActive(false);
        setRandomCallId('');
        setAudioLevel(0);
        console.log('ğŸ“ Ended private call');
    };

    // åˆ‡æ›éœéŸ³
    const toggleMute = () => {
        if (streamRef.current) {
            streamRef.current.getAudioTracks().forEach(track => {
                track.enabled = isMuted;
            });
            setIsMuted(!isMuted);
        }
    };

    return (
        <div className="space-y-4">
            {/* ç¾¤çµ„èªéŸ³ PTT */}
            <div className="bg-white rounded-lg border border-gray-200 p-4">
                <h4 className="font-semibold mb-3 flex items-center gap-2">
                    <Mic className="w-4 h-4" />
                    ç¾¤çµ„èªéŸ³ PTT
                </h4>

                <div className="space-y-3">
                    {/* ç•¶å‰é »é“ */}
                    <div className="text-sm text-gray-600">
                        ç•¶å‰é »é“: <span className="font-medium text-gray-900">{channel}</span>
                    </div>

                    {/* PTT æŒ‰éˆ• - æ”¹ç‚ºé»æ“Šé–‹å§‹/çµæŸ */}
                    <button
                        onClick={() => {
                            if (isRecording && !privateCallActive) {
                                stopGroupRecording();
                            } else if (!privateCallActive) {
                                startGroupRecording();
                            }
                        }}
                        disabled={privateCallActive}
                        className={`w-full py-4 rounded-lg font-semibold transition-all ${
                            isRecording && !privateCallActive
                                ? 'bg-red-600 text-white shadow-lg scale-105'
                                : 'bg-blue-600 text-white hover:bg-blue-700'
                        } disabled:bg-gray-300 disabled:cursor-not-allowed`}
                    >
                        {isRecording && !privateCallActive ? (
                            <div className="flex items-center justify-center gap-2">
                                <Mic className="w-5 h-5 animate-pulse" />
                                <span>é»æ“Šåœæ­¢ç™¼è©±</span>
                            </div>
                        ) : (
                            <div className="flex items-center justify-center gap-2">
                                <Mic className="w-5 h-5" />
                                <span>é»æ“Šé–‹å§‹ç™¼è©± (PTT)</span>
                            </div>
                        )}
                    </button>

                    {/* éŸ³è¨Šé›»å¹³æŒ‡ç¤ºå™¨ */}
                    {isRecording && !privateCallActive && (
                        <div className="space-y-1">
                            <div className="text-xs text-gray-600">éŸ³è¨Šé›»å¹³</div>
                            <div className="h-2 bg-gray-200 rounded-full overflow-hidden">
                                <div
                                    className="h-full bg-green-500 transition-all duration-100"
                                    style={{ width: `${audioLevel * 100}%` }}
                                />
                            </div>
                        </div>
                    )}

                    {/* å³æ™‚è½‰éŒ„æ–‡å­—é¡¯ç¤º */}
                    {isRecording && currentTranscript && (
                        <div className="bg-blue-50 border border-blue-200 rounded p-2">
                            <div className="text-xs text-gray-600 mb-1">æ­£åœ¨è­˜åˆ¥:</div>
                            <div className="text-sm text-blue-900 italic">{currentTranscript}</div>
                        </div>
                    )}

                    {/* éœéŸ³æŒ‰éˆ• */}
                    <button
                        onClick={toggleMute}
                        disabled={!isRecording}
                        className={`w-full px-4 py-2 rounded-lg text-sm font-medium transition-colors ${
                            isMuted
                                ? 'bg-red-100 text-red-700 border border-red-300'
                                : 'bg-gray-100 text-gray-700 border border-gray-300'
                        } disabled:opacity-50 disabled:cursor-not-allowed`}
                    >
                        {isMuted ? (
                            <span className="flex items-center justify-center gap-2">
                                <VolumeX className="w-4 h-4" />
                                å·²éœéŸ³
                            </span>
                        ) : (
                            <span className="flex items-center justify-center gap-2">
                                <Volume2 className="w-4 h-4" />
                                å–æ¶ˆéœéŸ³
                            </span>
                        )}
                    </button>
                </div>
            </div>

            {/* ç§äººé€šè©± */}
            <div className="bg-white rounded-lg border border-gray-200 p-4">
                <h4 className="font-semibold mb-3 flex items-center gap-2">
                    <Phone className="w-4 h-4" />
                    ç§äººé€šè©±
                </h4>

                <div className="space-y-3">
                    {!privateCallActive ? (
                        <>
                            <input
                                type="text"
                                value={privateTargetId}
                                onChange={(e) => setPrivateTargetId(e.target.value)}
                                placeholder="è¼¸å…¥ç›®æ¨™è¨­å‚™ ID"
                                className="w-full px-3 py-2 border border-gray-300 rounded-lg text-sm focus:ring-2 focus:ring-blue-500"
                            />
                            <button
                                onClick={startPrivateCall}
                                disabled={isRecording || !privateTargetId.trim()}
                                className="w-full px-4 py-2 bg-green-600 text-white rounded-lg hover:bg-green-700 disabled:bg-gray-300 disabled:cursor-not-allowed transition-colors"
                            >
                                <span className="flex items-center justify-center gap-2">
                                    <Phone className="w-4 h-4" />
                                    ç™¼èµ·é€šè©±
                                </span>
                            </button>
                        </>
                    ) : (
                        <div className="space-y-3">
                            <div className="text-sm">
                                <div className="text-gray-600">é€šè©±ä¸­</div>
                                <div className="font-medium">â†’ {privateTargetId}</div>
                                <div className="text-xs text-gray-500 mt-1">
                                    Call ID: {randomCallId}
                                </div>
                            </div>

                            {/* éŸ³è¨Šé›»å¹³ */}
                            {isRecording && (
                                <div className="space-y-1">
                                    <div className="text-xs text-gray-600">éŸ³è¨Šé›»å¹³</div>
                                    <div className="h-2 bg-gray-200 rounded-full overflow-hidden">
                                        <div
                                            className="h-full bg-blue-500 transition-all duration-100"
                                            style={{ width: `${audioLevel * 100}%` }}
                                        />
                                    </div>
                                </div>
                            )}

                            <button
                                onClick={endPrivateCall}
                                className="w-full px-4 py-2 bg-red-600 text-white rounded-lg hover:bg-red-700 transition-colors"
                            >
                                <span className="flex items-center justify-center gap-2">
                                    <PhoneOff className="w-4 h-4" />
                                    çµæŸé€šè©±
                                </span>
                            </button>
                        </div>
                    )}
                </div>
            </div>

            {/* èªéŸ³è½‰æ–‡å­—å’Œè‡ªå‹•æ–·å¥è¨­å®š */}
            <div className="bg-white rounded-lg border border-gray-200 p-4">
                <h4 className="font-semibold mb-3 flex items-center justify-between">
                    é€²éšè¨­å®š
                    {speechRecognitionEnabled && (
                        <span className="text-xs text-green-600 flex items-center gap-1">
                            <span className="w-2 h-2 bg-green-600 rounded-full animate-pulse"></span>
                            STT é‹è¡Œä¸­
                        </span>
                    )}
                </h4>

                <div className="space-y-3">
                    {/* èªéŸ³è­˜åˆ¥ç‹€æ…‹ */}
                    <div className="bg-blue-50 border border-blue-200 rounded p-2 text-xs">
                        <div className="font-semibold mb-1">èªéŸ³è½‰æ–‡å­—ç‹€æ…‹</div>
                        <div className="text-gray-700">
                            {speechRecognitionEnabled ? (
                                <span className="text-green-600">âœ… èªéŸ³è­˜åˆ¥å·²å•Ÿå‹•</span>
                            ) : (
                                <span className="text-gray-500">â¸ï¸ èªéŸ³è­˜åˆ¥æœªå•Ÿå‹•</span>
                            )}
                        </div>
                    </div>

                    {/* è‡ªå‹•ç™¼é€é–‹é—œ */}
                    <div className="flex items-center justify-between">
                        <label className="text-sm text-gray-700">è‡ªå‹•æ–·å¥ç™¼é€</label>
                        <input
                            type="checkbox"
                            checked={autoSend}
                            onChange={(e) => setAutoSend(e.target.checked)}
                            className="w-4 h-4"
                        />
                    </div>

                    {/* éœéŸ³é–¾å€¼ */}
                    <div>
                        <label className="text-sm text-gray-700 block mb-1">
                            éœéŸ³é–¾å€¼: {(silenceThreshold * 100).toFixed(0)}%
                        </label>
                        <input
                            type="range"
                            min="0"
                            max="0.1"
                            step="0.01"
                            value={silenceThreshold}
                            onChange={(e) => setSilenceThreshold(parseFloat(e.target.value))}
                            className="w-full"
                        />
                    </div>

                    {/* éœéŸ³æŒçºŒæ™‚é–“ */}
                    <div>
                        <label className="text-sm text-gray-700 block mb-1">
                            éœéŸ³æŒçºŒæ™‚é–“: {(silenceDuration / 1000).toFixed(1)}ç§’
                        </label>
                        <input
                            type="range"
                            min="500"
                            max="3000"
                            step="100"
                            value={silenceDuration}
                            onChange={(e) => setSilenceDuration(parseInt(e.target.value))}
                            className="w-full"
                        />
                    </div>

                    <div className="text-xs text-gray-600">
                        å•Ÿç”¨è‡ªå‹•æ–·å¥å¾Œï¼Œç³»çµ±æœƒåµæ¸¬éœéŸ³ä¸¦è‡ªå‹•ç™¼é€èªéŸ³ç‰‡æ®µ
                    </div>
                </div>
            </div>

            {/* ä½¿ç”¨èªªæ˜ */}
            <div className="bg-blue-50 border border-blue-200 rounded-lg p-3 text-xs text-gray-700">
                <div className="font-semibold mb-1">ä½¿ç”¨èªªæ˜</div>
                <ul className="space-y-1 list-disc list-inside">
                    <li>ç¾¤çµ„èªéŸ³ï¼šé»æ“Šã€Œé–‹å§‹ç™¼è©±ã€é–‹å§‹éŒ„éŸ³ï¼Œé»æ“Šã€Œåœæ­¢ç™¼è©±ã€çµæŸä¸¦ç™¼é€</li>
                    <li>ç§äººé€šè©±ï¼šè¼¸å…¥ç›®æ¨™ IDï¼Œé»æ“Šç™¼èµ·é€šè©±</li>
                    <li>é€šè©±ä¸­ç„¡æ³•ä½¿ç”¨ç¾¤çµ„ PTT</li>
                    <li>è‡ªå‹•æ–·å¥ï¼šå¯é–‹å•Ÿå¾Œç³»çµ±æœƒæ ¹æ“šéœéŸ³åµæ¸¬è‡ªå‹•åˆ†æ®µç™¼é€ï¼ˆé è¨­é—œé–‰ï¼‰</li>
                    <li>èªéŸ³è½‰æ–‡å­—ï¼šæ”¯æ´ç¹é«”ä¸­æ–‡å³æ™‚è½‰æ›ï¼ˆéœ€ç€è¦½å™¨æ”¯æ´ï¼‰</li>
                </ul>
            </div>
        </div>
    );
};

export default PTTAudio;
